{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dependencies required for the program to run include\n",
    "1.Numpy - Performing Mathematical Calculations\n",
    "2.MatPlotLib - Library for Ploting Graphs\n",
    "3.Pandas - For handling the data. Pandas provide many read-made features for reading csv, and handling data as a DataFrame object \n",
    "4.Seaborn\n",
    "5.Random\n",
    "6.Math\n",
    "7.Sklearn\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the Parameters for our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializer for an L-layer Neural Network.\n",
    "- The function `parameter_initialization` is a helper function which takes in input a list(dimensions) which contain information about the Layer (both hidden ,input and output layer included) and returns a params dictionary with the requisite size of the params in the given layer using a randomizer function\n",
    "- Additionally assert is used to make sure that the given parameter matrix matches in size with the dimensions dictionary\n",
    "- Each layer in the parameter dictionary is represented as Wn and bn where n indicates the nth layer of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initialization(dimensions):\n",
    "    \n",
    "    np.random.seed(4)\n",
    "    #Initializing the Parameters Dictionary\n",
    "    params = {}\n",
    "    #L is used to store the length of the total layers in the network\n",
    "    L = len(dimensions)            \n",
    "    #Next we iterate through each layer of the network , filling in the parameter of the network\n",
    "    for l in range(1, L):\n",
    "        #np.random.randn(x,y) creates a random array of size x,y drawn from normal distribution\n",
    "        #We further multiply this matrix with a value of 0.01 to keep the values of the Weights small in the initial iterations of the network\n",
    "        params['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1])*0.01\n",
    "        params['b' + str(l)] = np.zeros((dimensions[l],1))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the params in place ,we now perform forward propagation . \n",
    "\n",
    "First we start of by defining `forward_prop_lin` module which computes the following equations:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n",
    "\n",
    "Next we code `sigmoid` function which performs the $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. operation on the individual elements of the matrix Z.\n",
    "\n",
    "Also for comparitive purpose we also code `relu` function doing $A = RELU(Z) = max(0, Z)$ . In the later sections of the notebook we ran the code using both relu and sigmoid and have drawn a comparitive study of both the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_lin(A, W, b):\n",
    "    #Z is the input of the activation function which is calculated from the given formula\n",
    "    Z = np.dot(W,A) + b\n",
    "    #temp_backprop is a python dictionary , which will be required later in the backward pass\n",
    "    temp_backprop= (A, W, b)\n",
    "    \n",
    "    return Z,temp_backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    #Using Sigmoid Activator , the function has been described in detail in the markdown\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    temp_backprop= Z\n",
    "    return A,temp_backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    #Using Relu Activator, the function has been described in detail in the markdown\n",
    "    A = np.maximum(0,Z)\n",
    "    temp_backprop= Z \n",
    "    return A,temp_backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the linear forward model in place, we next code up the two helper functions `forward_prop_activation` and `forward_prop_full`.\n",
    "\n",
    "- `forward_prop_activation` takes in input Activations of the previous layer, Weights and bias and applies the requisite activation function i.e sigmoid or relu activator and returns the Activation for the next layer. Also in the process we store thelist_tempbecause they'd be required for future back propagations\n",
    "\n",
    "- `forward_prop_full` this is the main function , this function sums up the entire forward pass for a training example in the dataset taking in X and params for the entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_activation(A_prev, W, b, activation):\n",
    "    #There are two choice of activations in this code namely sigmoid and relu\n",
    "    if activation == \"sigmoid\":\n",
    "        #If sigmoid is selected we first apply forward_prop_lin to find the pre-activation parameter Z and then apply the sigmoid activator\n",
    "        Z, linear_temp = forward_prop_lin(A_prev, W, b)\n",
    "        A, activation_temp = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        #If relu is selected we first apply forward_prop_lin to find the pre-activation parameter Z and then apply the relu activator\n",
    "        Z, linear_temp = forward_prop_lin(A_prev, W, b)\n",
    "        A, activation_temp = relu(Z)\n",
    "\n",
    "    temp_backprop= (linear_temp, activation_temp)\n",
    "    #Finally we return the new Activation Parameter for the next layer alongside thelist_tempfor propagation\n",
    "    return A,temp_backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_full(X, params):\n",
    "    #list_temp for the history of Activations\n",
    "    list_temp= []\n",
    "    #Activation A[0] is X in the Base Case\n",
    "    A = X\n",
    "    #Since params has 2 params for each Layers . Length of Parameter // 2  returns the Length of the Layer\n",
    "    L = len(params) // 2           \n",
    "    \n",
    "    #Iterating through the individual layers applying Relu Activators for L-1 Layers\n",
    "    for l in range(1, L):\n",
    "        #A prev = A[L-1]\n",
    "        A_prev = A \n",
    "        #Here activations of the individual layers are propagated in the iterative fasion using relu activators with the params as specified in the params dictionary\n",
    "        A,temp_backprop= forward_prop_activation(A_prev, params['W' + str(l)], params['b' + str(l)], \"sigmoid\")\n",
    "        list_temp.append(temp_backprop)\n",
    "        \n",
    "    #The last layer of the network is a sigmoid activator   \n",
    "    AL,temp_backprop= forward_prop_activation(A, params['W' + str(L)], params['b' + str(L)], \"sigmoid\")\n",
    "    list_temp.append(temp_backprop)\n",
    "    #Finally we return the Last Level of Activation alongside the list_temp\n",
    "            \n",
    "    return AL, list_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "Before Coding up BackPropagation we need to define some Cost Function to check if your model is actually learning  for this purpose we compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL, Y):\n",
    "    #Y.shape[1] returns the no of training example\n",
    "    m = Y.shape[1]\n",
    "    #np.multiply is used for the element wise multiplication of the vector\n",
    "    #longprobs vector is ensured that it has the same shape as Y as we have done element wise multiplication\n",
    "    mul_temp = np.multiply(np.log(AL),Y) + np.multiply(np.log(1-AL),1-Y)\n",
    "    #The cost across various examples are summed up to a single cost value\n",
    "    cost = (-1/m)*np.sum(mul_temp, axis = 1)\n",
    "    cost = np.squeeze(cost)      \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated the Cost function , using Cross Entropy Losses. We proceed with the Backpropagation of the Layers , with the updation of the params.\n",
    "\n",
    "Similiar to Forward Propagation we start of by defining `backward_prop_lin` module which computes the following equations:\n",
    "\n",
    "Assuming that we have already calculated $dZ$. The outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$.\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n",
    "\n",
    "Next we code `derivative_sigmoid` function which performs the \n",
    "```python\n",
    "dZ = derivative_sigmoid(dA, activation_temp)\n",
    "```\n",
    "And also `derivative_relu` function which performs\n",
    "```python\n",
    "dZ = derivative_relu(dA, activation_temp)\n",
    "```\n",
    "If $f(.)$ is the activation function, \n",
    "`derivative_sigmoid` and `derivative_relu` compute $$dZ^{[l]} = dA^{[l]} * f'(Z^{[l]}) \\tag{11}$$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop_lin(dZ,temp_backprop):\n",
    "    #The values of the Previous Activators , Weights and Bias are recovered fromtemp_backprop\n",
    "    A_prev, W, b =temp_backprop\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    #The three outputs are calculated using the above formulas\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_relu(dA,temp_backprop):\n",
    "    #Z recovered fromtemp_backprop\n",
    "    Z =temp_backprop\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    #DZ for ReLU is straight forward , giving zero for Z<=0 and dA for Z>=0\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(dA,temp_backprop):\n",
    "    #Z recovered fromtemp_backprop\n",
    "    Z =temp_backprop\n",
    "    #DZ for Sigmoid \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similiar to the Linear Forward Model we now code up the two helper functions `backward_prop_activation` and `backward_prop_full`.\n",
    "\n",
    "- `backward_prop_activation` takes in input change in activations of the previous layer,temp_backpropand activation  and applies the requisite activation_backward function by first computing dZ followed by backward_prop_lin function\n",
    "\n",
    "- `backward_prop_full` this is the main function , this function sums up the entire backward pass for a training example in the dataset taking in X and params for the entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop_activation(dA,temp_backprop, activation):\n",
    "    #Retrieving the linear_temp and activation_temp from thetemp_backprop\n",
    "    linear_temp, activation_temp =temp_backprop\n",
    "    #Depending upon the activation function speified in the activation we perform a backward pass on the data\n",
    "    if activation == \"relu\":\n",
    "        #if the activator used was relu we first compute Dz from derivative_relu pass and then use backward_prop_lin on the Dz Parameter thus obtained\n",
    "        dZ = derivative_relu(dA, activation_temp)\n",
    "        dA_prev, dW, db = backward_prop_lin(dZ, linear_temp)\n",
    "    \n",
    "    elif activation == \"sigmoid\":\n",
    "        #if the activator used was sigmoid we first compute Dz from derivative_sigmoid pass and then use backward_prop_lin on the Dz Parameter thus obtained\n",
    "        dZ = derivative_sigmoid(dA, activation_temp)\n",
    "        dA_prev, dW, db = backward_prop_lin(dZ, linear_temp)\n",
    " \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop_full(AL, Y, list_temp):\n",
    "    \"\"\"\n",
    "    The final function that sums up the enter backpropagation model\n",
    "    1. gradients = a dictionary with the gradients which will be used for parameter updation\n",
    "    2. L = no of layers in the network\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    L = len(list_temp) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    #To compute DL/DA for rhe first Back Propagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    current_temp = list_temp[L-1]\n",
    "    #Updation of the gradient of the final layer which happens to have a sigmoid activator\n",
    "    gradients[\"dA\" + str(L-1)], gradients[\"dW\" + str(L)], gradients[\"db\" + str(L)] = backward_prop_activation(dAL, current_temp, \"sigmoid\")\n",
    "\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        #Next we iterate through individual Layers of the network calculating the gradients at corresponding layers which are sigmoid activated \n",
    "        current_temp = list_temp[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_prop_activation(gradients[\"dA\" + str(l+1)], current_temp, \"sigmoid\")\n",
    "        gradients[\"dA\" + str(l)] = dA_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    #Here in this step the gradient is finally returned for parameter updation\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Updation\n",
    "With one pass of Forward and Backward Completed , we conclude the pass by Updating the params of each individual layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_update(params, gradients, learning_rate):\n",
    "    L = len(params) // 2\n",
    "    # number of layers in the neural network\n",
    "    # Iterating through Individual params and updating them with the corresponding gradients multiplied by learning_rate\n",
    "    for l in range(L):\n",
    "        params[\"W\" + str(l + 1)] = params[\"W\" + str(l + 1)] - learning_rate * gradients[\"dW\" + str(l + 1)]\n",
    "        params[\"b\" + str(l + 1)] = params[\"b\" + str(l + 1)] - learning_rate * gradients[\"db\" + str(l + 1)]\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Batch Model \n",
    "Having coded all the precomponents of the Network, finally we assemble all of them to form a full fledged network. The function `neural_network_batch` provides the implementation for the batch modeling of the Network. In this, the batch_size is set by the user. The errors are updated only after the processing of a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_batch(X, Y, layers_dims, learning_rate = 0.1, num_iterations = 3000, print_cost=False,batch_size=32):\n",
    "    \n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs, accu = [],[]                         \n",
    "    #keep track of cost\n",
    "    #batch_size=8\n",
    "    num_batches = X.shape[1]//batch_size\n",
    "    print(\"number of batches: \" + str(num_batches))\n",
    "    #Initialize the params for the network using the parameter_initialization helper function\n",
    "    params = parameter_initialization(layers_dims)\n",
    " \n",
    "    #Iterating throught the whole set num_iterations times, correcting gradients at each level and also collecting the cost to review them later \n",
    "    for i in range(0, num_iterations):\n",
    "        #For every batch of training example we run the algorithm\n",
    "        for k in range(0,num_batches):\n",
    "            #X_lite and Y_lite contains batch_size entries of X and Y respectively  for training purpose\n",
    "            X_lite = X[:,k*batch_size:(k+1)*batch_size]\n",
    "            Y_lite = Y[:,k*batch_size:(k+1)*batch_size]            \n",
    "             \n",
    "            #One pass of Forward Propagation\n",
    "            AL,list_temp= forward_prop_full(X_lite, params)\n",
    "            \n",
    "            if(k==0):\n",
    "                AL_lite = AL\n",
    "            else:\n",
    "                AL_lite = np.concatenate((AL_lite,AL),axis=1)\n",
    "          \n",
    "            #Computing the Cost function\n",
    "            cost = cost_function(AL, Y_lite)\n",
    "           \n",
    "        \n",
    "            #One pass of Backward Propagation\n",
    "            gradients = backward_prop_full(AL, Y_lite, list_temp)\n",
    "            \n",
    "            #Updation of params\n",
    "            params = params_update(params, gradients, learning_rate)\n",
    "        \n",
    "            \n",
    "\n",
    "        temp_array = AL_lite\n",
    "        p = np.zeros((1,temp_array.shape[1]))\n",
    "        for j in range(0, temp_array.shape[1]):\n",
    "            if temp_array[0,j] > 0.5:\n",
    "                p[0,j] = 1\n",
    "            else:\n",
    "                p[0,j] = 0\n",
    "        \n",
    "        Y_acc = Y[:,0:num_batches*batch_size]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            acc1 = np.sum((p == Y_acc)/AL_lite.shape[1])\n",
    "            print (\"Accuracy after iteration %i: %f\" %(i, acc1))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "            accu.append(acc1)\n",
    "    print (\"Training Accuracy: %f\" %(acc1))\n",
    "    # plot the cost\n",
    "    #plt.subplot(1, 2, 1)     \n",
    "    plt.plot(np.squeeze(costs),label='cost')\n",
    "    plt.plot(np.squeeze(accu),label='accuracy')\n",
    "    plt.ylabel('metrics')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.legend()\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "With the trained model, we are in a position to start making predictions about the independent, unseen training data to assess the credibility of the model. The metric used for the same, is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, params):\n",
    "    \"\"\"\n",
    "    To Predict the label , given the dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1] \n",
    "    n = len(params) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # With the model trained , we do one forward pass of the model and check the probability of the label\n",
    "    temp_array,list_temp= forward_prop_full(X, params)\n",
    "\n",
    "    \n",
    "    #If the probability of any label is greater than 0.5 our model predicts it as 1 or 0\n",
    "    for i in range(0, temp_array.shape[1]):\n",
    "        if temp_array[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    #As we already know the correct value of the output we can also calculate the accuracy of the model\n",
    "    print(\"Test Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"paper_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = data1['Class']\n",
    "X1 = data1.drop(['Class'], 1)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.2, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25820, 26)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train = X1_train.T\n",
    "X1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [X1_train.shape[0], 16, 8, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 26)\n",
      "number of batches: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/user/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.304430\n",
      "Accuracy after iteration 0: 0.846154\n",
      "Cost after iteration 10: 0.218935\n",
      "Accuracy after iteration 10: 0.846154\n",
      "Training Accuracy: 0.846154\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X1_train1 = np.asarray(X1_train)\n",
    "y1_train1 = np.asarray(y1_train)\n",
    "y1_train1 = y1_train1.reshape(1, y1_train1.shape[0])\n",
    "print(y1_train1.shape)\n",
    "params = neural_network_batch(X1_train1, y1_train1, layers_dims, learning_rate = 0.1, num_iterations = 20, print_cost = True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = X1_test.T\n",
    "X1_test = np.asarray(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_test = np.asarray(y1_test)\n",
    "y1_test = y1_test.reshape(1, y1_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.857142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "pred_test1 = predict(X1_test, y1_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
